{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JM_ErGq-k_3c"
      },
      "source": [
        "# Group Members:\n",
        "\n",
        "*   Nicolás Vila Alarcón (240230)\n",
        "*   David Pérez Carrasco (241375)\n",
        "*   Guillem Escriba Molto (242123)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sf3OE-KdlByS"
      },
      "source": [
        "# Lab 2 Assignment"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rM-59dldGpkD",
        "outputId": "1db92a30-430b-46cc-9380-3176aff42acf"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JpkW3zxtGo0f",
        "outputId": "1b80a90a-74ec-45b8-98d1-97a3ae7b4d6a"
      },
      "outputs": [],
      "source": [
        "%cd \"/content/drive/MyDrive\"\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h-tLxR9EGlep"
      },
      "source": [
        "# **Practical session 2: an image denoising energy**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ndG1DGqaGlex"
      },
      "source": [
        "## **1. Gradient descent**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K005GW8yGlex"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from PIL import Image\n",
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "\n",
        "from mpl_toolkits.mplot3d import Axes3D\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib import cm\n",
        "from IPython import display\n",
        "from math import sqrt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s_58fnA6Gley"
      },
      "source": [
        "### **1. Complete the MatLab functions toy_fun and toy_gradient. These functions implement the function f and its gradient.**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S1VqvDiDGlez"
      },
      "source": [
        "**toy_fun**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "04Zac59DU-Dp"
      },
      "source": [
        "\\begin{equation}\n",
        "f(x_1,x_2) = \\frac1{1000}\\left(x_1^4 + x_2^4 - 80 x_1^2 - 60 x_2^2 + 100x_1 +\n",
        "50 x_2 + 1\\right)\n",
        "\\end{equation}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iu4TQq6cGlez"
      },
      "outputs": [],
      "source": [
        "def toy_fun(x: np.matrix):\n",
        "    \"\"\"\n",
        "    Polynomial toy function - see the guide\n",
        "    \n",
        "    :param x: 2x1 vector \n",
        "    \n",
        "    :return y: value of the function at point x\n",
        "    \"\"\"\n",
        "    # TODO: Compute the function\n",
        "    y = 1/1000*(pow(x[0],4)+pow(x[1],4)-80*pow(x[0],2)-60*pow(x[1],2)+100*x[0]+50*x[1]+1)\n",
        "    return y"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9sFA0GjGTvfy",
        "outputId": "26d2f57a-9bbc-4a2c-84ce-780273687e9b"
      },
      "outputs": [],
      "source": [
        "x = np.matrix([[1],[2]])\n",
        "y = toy_fun(x)\n",
        "print(y)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gNg3oY61Gle0"
      },
      "source": [
        "**toy_gradient**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jLA5PY1VVHWA"
      },
      "source": [
        "We know that:\n",
        "\n",
        "$$\\nabla f(x_{1},x_{2}) =\n",
        "\\begin{bmatrix}\n",
        "\\dfrac{\\partial f(x_{1},x_{2})}{\\partial x_{1}}\\\\\n",
        "\\\\\n",
        "\\dfrac{\\partial f(x_{1},x_{2})}{\\partial x_{2}}\n",
        "\\\\\n",
        "\\end{bmatrix} =\n",
        "\\begin{bmatrix}\n",
        "\\frac1{1000}\\left(4x_1^3 - 160 x_1 + 100\\right)\\\\\n",
        "\\\\\n",
        "\\frac1{1000}\\left(4x_2^3 - 120 x_2 + 50\\right)\n",
        "\\\\\n",
        "\\end{bmatrix}$$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DmqASbCOGle0"
      },
      "outputs": [],
      "source": [
        "def toy_gradient(x: np.matrix):\n",
        "    \"\"\"\n",
        "    Gradient of toy_fun polynomial toy function \n",
        "    \n",
        "    :param x: 2x1 matrix \n",
        "    :return grad: 2x1 matrix: gradient of the toy function at point x\n",
        "    \"\"\"\n",
        "    # TODO: Compute the gradient of the toy function (must be calculated by hand)\n",
        "    grad = np.matrix([[1/1000*(4*pow(x[0,0],3)-160*x[0,0]+100)],[1/1000*(4*pow(x[1,0],3)-120*x[1,0]+50)]])\n",
        "    return grad"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cw_vPYXCXGcL",
        "outputId": "ea9ac55c-980a-4f52-c2e5-8838bc3663af"
      },
      "outputs": [],
      "source": [
        "#x = np.array([1,2])\n",
        "x = np.matrix([1, 2])\n",
        "grad = toy_gradient(x.T)\n",
        "print(grad)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nTGNASsYGle1"
      },
      "source": [
        "### **2. Complete the MatLab function gradient_descent. This function implements a gradient descent algorithm. We are going to implement it in a way in which we can use the same gradient descent function for this toy example and for the denoising energy of the next section. Follow the comments provided in the code.**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I1ZgrgcRGle1"
      },
      "outputs": [],
      "source": [
        "def gradient_descent(callback,\n",
        "                     callback_params: dict, \n",
        "                     initial_condition: np.matrix, \n",
        "                     step_size: float,\n",
        "                     max_iterations: int,\n",
        "                     tolerance: float,\n",
        "                     fig = None,\n",
        "                     ax = None):\n",
        "    \"\"\"\n",
        "    Implementation of the gradient descent algorithm with\n",
        "    fixed step size. It uses function handles (handles are MatLab pointers). It\n",
        "    can work with any function and gradient if they are implemented with . Here params\n",
        "    is a structure with the internal parameters of my_fun and my_grad.\n",
        "\n",
        "    :param callback: gradient of function to be optimized\n",
        "    :param callback_params: a structure with the internal parameters of the target function and \n",
        "                            its gradient.\n",
        "    :param initial_condition: initial condition for gradient descent\n",
        "    :param step_size: size of the gradient descent steps\n",
        "    :param max_iterations: maximum number of iterations\n",
        "    :param tolerance: tolerance for the stopping condition (it stop when \n",
        "                      the norm of the gradient is below the tolerance)\n",
        "\n",
        "    :return current_value: value found\n",
        "    \"\"\"\n",
        "    # Initialize variables\n",
        "    current_value = initial_condition\n",
        "    previous_value = current_value\n",
        "    current_iteration = 0\n",
        "    current_norm_value = np.inf\n",
        "    xk = {}\n",
        "\n",
        "    xk[0]=current_value;\n",
        "    # Main loop for Gradient Descent\n",
        "    while (current_norm_value > tolerance) and (current_iteration < max_iterations):\n",
        "        # Keep previous - just for visualization\n",
        "        \n",
        "        previous_value = current_value\n",
        "\n",
        "        # TODO: Run the gradient descent\n",
        "        gf = np.matrix([])\n",
        "        gf = callback(previous_value, **callback_params)\n",
        "        \n",
        "        # TODO: Update the current value and norm value\n",
        "        current_value = np.matrix([])\n",
        "        current_value = previous_value - step_size*gf\n",
        "        xk[current_iteration+1]=current_value;\n",
        "\n",
        "        current_norm_value = np.matrix([])\n",
        "        current_norm_value = np.linalg.norm(gf)\n",
        "        #print(\"{} of {} -> tolerance: {}\".format(current_iteration+1, max_iterations, current_norm_value))\n",
        "        \n",
        "        # Plot current position! Just for visualization purposes \n",
        "        # if x is a 2x1 vector (visualization of toy example) \n",
        "        if (current_value.shape[0] == 2) & (current_value.shape[1] == 1):\n",
        "            if not ax:\n",
        "                fig, ax = plt.subplots()\n",
        "            ax.plot(current_value[1, 0], current_value[0, 0], marker = 'o', color = \"k\")\n",
        "            ax.plot([previous_value[1, 0], current_value[1, 0]], \n",
        "                    [previous_value[0, 0], current_value[0, 0]], \"-k\")\n",
        "            display.clear_output(wait=True)\n",
        "            display.display(fig)            \n",
        "\n",
        "        # Update the iteration\n",
        "        current_iteration += 1\n",
        "        \n",
        "    return current_value, xk"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fYVlFqz2Gle3"
      },
      "source": [
        "### **3 Run the following block and answer the questions on the PDF**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wCyMh-1jGle3"
      },
      "outputs": [],
      "source": [
        "def toy_main():\n",
        "    # Set the grids\n",
        "    x1 = np.arange(-10, 10, 0.1)\n",
        "    x2 = np.arange(-10, 10, 0.1)\n",
        "\n",
        "    # Evaluate the toy_fun\n",
        "    y = np.zeros(shape = (len(x1), len(x2)))\n",
        "    for i, x1_value in enumerate(x1):\n",
        "        for j, x2_value in enumerate(x2):\n",
        "            y[i, j] = toy_fun([x1_value, x2_value])\n",
        "\n",
        "    # Plot the surface.\n",
        "    fig = plt.figure(figsize = (14, 8))\n",
        "    ax = fig.gca(projection='3d')\n",
        "    X, Y = np.meshgrid(x1, x2)\n",
        "    Z = y\n",
        "    surf = ax.plot_surface(X, Y, Z, cmap=cm.coolwarm,\n",
        "                           linewidth=0, antialiased=False)\n",
        "\n",
        "    # Add a color bar which maps values to colors.\n",
        "    fig.colorbar(surf, shrink=0.5, aspect=5)\n",
        "\n",
        "    # Show the surface\n",
        "    #plt.show()\n",
        "\n",
        "    # Plot the contours\n",
        "    fig_contours, ax_contours = plt.subplots(figsize = (10, 8))\n",
        "    CS = ax_contours.contour(X, Y, Z, \n",
        "                    corner_mask = False, levels = 150, \n",
        "                    linewidths=(1,), cmap = cm.coolwarm)\n",
        "    \n",
        "    # Set initial condition --> TRY CHANGING IT\n",
        "    x0 = np.matrix([[5], [-5]])\n",
        "    \n",
        "    # Set gradient descent parameters --> EXPLORE USING DIFFERENT PARAMETERS\n",
        "    step_size= 1\n",
        "    tolerance = 0.01\n",
        "    max_iterations = 200\n",
        "\n",
        "    # Call gradient descent minimization\n",
        "    print(\"First gradient descent ...\")\n",
        "    xs_1, xk_1 = gradient_descent(callback = toy_gradient,\n",
        "                            callback_params = {}, \n",
        "                            initial_condition = x0, \n",
        "                            step_size = step_size,\n",
        "                            max_iterations = max_iterations,\n",
        "                            tolerance = tolerance,\n",
        "                            fig = fig_contours,\n",
        "                            ax = ax_contours)\n",
        "    \n",
        "    err1 = [None]*len(xk_1)\n",
        "    for i in range(len(xk_1)):\n",
        "      err1[i]=sqrt(pow((xk_1[i]-xs_1)[0],2)+pow((xk_1[i]-xs_1)[1],2))\n",
        " \n",
        "\n",
        "    # Set gradient descent parameters --> EXPLORE USING DIFFERENT PARAMETERS\n",
        "    step_size= 5\n",
        "    tolerance = 0.1\n",
        "    max_iterations = 100\n",
        "\n",
        "    # Call gradient descent minimization\n",
        "    print(\"\\nSecond gradient descent ...\")\n",
        "    xs_2, xk_2 = gradient_descent(callback = toy_gradient,\n",
        "                            callback_params = {}, \n",
        "                            initial_condition = x0, \n",
        "                            step_size = step_size,\n",
        "                            max_iterations = max_iterations,\n",
        "                            tolerance = tolerance,\n",
        "                            fig = fig_contours,\n",
        "                            ax = ax_contours)\n",
        "    \n",
        "    err2 = [None]*len(xk_2)\n",
        "    for i in range(0,len(xk_2)):\n",
        "      err2[i]=sqrt(pow((xk_2[i]-xs_2)[0],2)+pow((xk_2[i]-xs_2)[1],2))\n",
        "\n",
        "    return  err1,err2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "r6onnRAWGle5",
        "outputId": "02e0fcbe-90e5-47a6-e4f2-5c237026c828"
      },
      "outputs": [],
      "source": [
        "e1,e2 = toy_main()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 279
        },
        "id": "6tsKFPMVhQSX",
        "outputId": "e3b4c8db-3e3e-4868-cfee-5099156e15cc"
      },
      "outputs": [],
      "source": [
        "x= [None]*len(e1)\n",
        "for i in range(len(e1)):\n",
        "  x[i]=i\n",
        "plt.plot(x,e1)\n",
        "plt.ylabel('error (||x_k-x*||)')\n",
        "plt.xlabel('iteration (k)')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 279
        },
        "id": "QDIhBM3HhQfD",
        "outputId": "58d26305-ffb7-4bc5-88cf-ac60445d1916"
      },
      "outputs": [],
      "source": [
        "x= [None]*len(e2)\n",
        "for i in range(len(e2)):\n",
        "  x[i]=i\n",
        "plt.plot(x,e2)\n",
        "plt.ylabel('error (||x_k-x*||)')\n",
        "plt.xlabel('iteration (k)')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IBJAkfYeFpfF"
      },
      "source": [
        "\n",
        "1.   In total, as we can see in the plot above, there are 4 local minima.\n",
        "\n",
        "2.   When we set an initial value x = (-2, -8) the algorithm also converges to a minimum but a different one than when we used x0 = (5, 1). We can also notice that with the step size of 5 the followed path is different from the one with step size of 1, which did not happen with the previous point. We also see that it is faster with a step size of 5. As we can see, it actually reaches the global minima, which, as we can comprove from the colours of the plot, it is placed at aproximately (-6,25,-6,25). \n",
        "\n",
        "3. In the last plots we see the convergence of the error of each execution of the gaussian, first with step_size=1 and then with step_size=5, so we can notice a clear logarithm function, especially in the first one, because the second one needs just 3 steps so the plot is not smooth, which makes sense because with step_size being 5 is faster to reach the point. We know from theory that the convergence is, in the worst case, linear, so we see both executions of this are better than linear because the first one is logarithmical and the second one is similar to that. We know that the logarithm of the error follows a expression like the following: log ek = k log r + log e0, so we just need to estimate the average value of r by computing this equation with each value of ek. In the first case we obtain a value of r similar to 0.7, while in the second one we obtain a value of just 0.045, which shows the great difference between both estimators, as the second one only needs 2 iterations to reach the point and so its error is almost insignificant.\n",
        "\n",
        "4. From testing with different values, we have noticed that, depending on the starting point and the closer local minima, the path will be quicker with a step size closer to 5, but if we raise this value too much, for example 10, we'd get the answer really slowly, in the case we do, and the opposite happens with a smaller step size, for example 0.01, whith which the path advances really slowly and the minima is probably never reached. So the faster and more accurate step size are in general the medium values, for instance 5.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v8Dik7oYGle6"
      },
      "source": [
        "## **2. Image denoising energy**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pd5F9np2Gle6"
      },
      "source": [
        "### **1. Complete the functions _im_fwd_gradient_ and _im_bwd_divergence_. These functions compute the forward gradient &nabla;<sup>+</sup> and the backwards divergence div<sup>-</sup>. Follow the comments provided in the code. Avoid building the matrices &nabla;<sup>+</sup> and div<sup>-</sup>**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rNOgE1ZbGle6"
      },
      "source": [
        "#### **1.1 im_fwd_gradient**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LQh9TUgD1C1S"
      },
      "source": [
        "For matrix $u$, $\\nabla^{+}u_{i,j} = (\\nabla^{+}_{i}u_{i,j},\\nabla^{+}_{j}u_{i,j})$ where:\n",
        "\n",
        "$$\\nabla^{+}_{i}u_{i,j} = \\begin{cases}\n",
        "  u_{i+1,j} - u_{i,j} & \\text{if }i<M\\\\ \n",
        "  0& \\text{if }i=M\n",
        "\\end{cases}$$\n",
        "\n",
        "$$\\nabla^{+}_{j}u_{i,j} = \\begin{cases}\n",
        "  u_{i,j+1} - u_{i,j} & \\text{if }j<N\\\\ \n",
        "  0& \\text{if }j=N\n",
        "\\end{cases}\n",
        "$$\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tPPwi30VGle7"
      },
      "outputs": [],
      "source": [
        "def im_fwd_gradient(image: np.matrix):\n",
        "    \"\"\"\n",
        "    Discrete gradient of an image using forward differences, with homogeneous Neuman boundary conditions.\n",
        "\n",
        "    :param u: image (MxN)\n",
        "            \n",
        "    :return gradu_j: partial derivative in the j (rows) direction (also x direction)\n",
        "    :return gradu_i: partial derivative in the i (cols) direction (also y direction)\n",
        "    \"\"\"\n",
        "    # TODO: Get the size of the image\n",
        "    image_shape = image.shape\n",
        "    M = image_shape[0]\n",
        "    N = image_shape[1]\n",
        "    \n",
        "    gradu_i = np.zeros((M, N))\n",
        "    gradu_j = np.zeros((M,N))\n",
        "\n",
        "    for i in range(M):\n",
        "      for j in range(N):\n",
        "        if i ==M-1:\n",
        "          gradu_i[i,j] = 0\n",
        "        else:\n",
        "          gradu_i[i,j] = image[i+1,j]-image[i,j]\n",
        "          \n",
        "    for i in range(M):\n",
        "      for j in range(N):\n",
        "            if j == N-1:\n",
        "              gradu_j[i,j] = 0\n",
        "            else:\n",
        "              gradu_j[i,j] = image[i,j+1]-image[i,j]\n",
        "\n",
        "    return gradu_i, gradu_j"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TFd77jw9Gle7"
      },
      "source": [
        "#### **1.2 im_bwd_divergence**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WS98wPDS1DYy"
      },
      "source": [
        "For matrix $u$, with $u^{1} = \\nabla^{+}_{i}u_{i,j}$ and $u^{2} = \\nabla^{+}_{j}u_{i,j}$:\n",
        "\n",
        "$$(div^{-}\\,u) = \n",
        "\\begin{cases}\n",
        "  u^{1}_{i,j} - u^{1}_{i-1,j} & \\text{if }1< i<M\\\\ \n",
        "  u^{1}_{i,j} & \\text{if }i=1\\\\\n",
        "  - u^{1}_{i-1,j} & \\text{if }i=M\n",
        "\\end{cases}\n",
        "+\n",
        "\\begin{cases}\n",
        "  u^{2}_{i,j} - u^{2}_{i,j-1} & \\text{if }1< j<N\\\\ \n",
        "  u^{2}_{i,j} & \\text{if }j=1\\\\\n",
        "  - u^{2}_{i,j-1} & \\text{if }j=N\n",
        "\\end{cases}\n",
        "$$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lHdjZJ_-Gle8"
      },
      "outputs": [],
      "source": [
        "def im_bwd_divergence(gradient_i: np.matrix,\n",
        "                      gradient_j: np.matrix):\n",
        "    \"\"\"\n",
        "    Discrete divergence of a vector field using backwards differences. \n",
        "    This is the negative transpose of the im_fwd_gradient\n",
        "    \n",
        "    :param gradient_i: component of g in the direction j (rows) (also x direction)\n",
        "    :param gradient_j: component of g in the direction i (cols) (also y direction)\n",
        "    \n",
        "    :return divg: backwards divergence of g\n",
        "    \"\"\"\n",
        "    M = gradient_i.shape[0]\n",
        "    N = gradient_i.shape[1]\n",
        "    divg = np.zeros((M,N))\n",
        "\n",
        "    for i in range(M):\n",
        "      for j in range(N):\n",
        "        if i == 0:\n",
        "          divg[i,j] += gradient_i[i,j]\n",
        "\n",
        "        if j == 0:\n",
        "          divg[i,j] += gradient_j[i,j]\n",
        "\n",
        "        if i == M-1:\n",
        "          divg -= gradient_i[i-1,j]\n",
        "\n",
        "        if j == N-1:\n",
        "          divg -= gradient_j[i,j-1]\n",
        "\n",
        "        if i<M-1 and i>0:\n",
        "          divg[i,j] += (gradient_i[i, j] - gradient_i[i-1, j])\n",
        "\n",
        "        if j<N-1 and j>0:\n",
        "          divg[i,j] += gradient_j[i, j] - gradient_j[i, j-1] \n",
        "    \n",
        "    return divg"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qd0UNkUlGle8"
      },
      "source": [
        "### **2. Complete the functions denoise_energy and denoise_energy_gradient following the comments provided in the code. Avoid building sparse huge matrices.**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H_qwqzQjV1GX"
      },
      "source": [
        "Let:\n",
        "\n",
        "+ $u \\rightarrow$ Decision variable (final image)\n",
        "+ $f \\rightarrow$ Noisy Image\n",
        "\n",
        "Then,\n",
        "\n",
        "\\begin{equation}\n",
        "\tE(u) = \\overbrace{\\sum_{i = 1}^M\\sum_{j = 1}^N c_{ij}|\\nabla^+u_{ij}|^2}^{\\text{regularization}}  +\t\\beta \\overbrace{\\sum_{i = 1}^M\\sum_{j = 1}^N (u_{ij} -\tf_{ij})^2,}^{\\text{data attachment}}\n",
        "\\end{equation}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z5Pv36snGle8"
      },
      "outputs": [],
      "source": [
        "def denoise_energy(image: np.matrix,\n",
        "                   noise: np.matrix,\n",
        "                   coefficients: np.matrix,\n",
        "                   beta: float):\n",
        "    \"\"\"\n",
        "    Evaluates the denoising energy from an image and the noisy\n",
        "    data (see the guide)\n",
        "\n",
        "    :param  image: target image (MxN)\n",
        "    :param  noise: (MxN) noisy data for attachment term \n",
        "    :param  coefficients: (MxN) coefficients image for regularization term\n",
        "    :param  beta: (1x1) weight of attachment term\n",
        "\n",
        "    :return e: energy value\n",
        "    \"\"\"\n",
        "    energy = 0.0\n",
        "    \n",
        "    image_shape = image.shape\n",
        "    M = image_shape[0]\n",
        "    N = image_shape[1]\n",
        "    \n",
        "\n",
        "    # TODO: Calculate the regularization term\n",
        "    reg = 0.0\n",
        "    grad_i, grad_j = im_fwd_gradient(image)\n",
        "    for i in range(M):\n",
        "      for j in range(N):\n",
        "        reg += coefficients[i,j]* (pow(grad_i[i,j],2)+pow(grad_j[i,j],2))\n",
        "    \n",
        "    # TODO: Calculate the data attachment term\n",
        "    dat = 0.0\n",
        "    for i in range(M):\n",
        "      for j in range(N):\n",
        "        dat += beta*pow(image[i,j]-noise[i,j], 2)\n",
        "\n",
        "\n",
        "    energy = reg + dat\n",
        "\n",
        "    return energy"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AdkShepzV9Cg"
      },
      "source": [
        "Also,\n",
        "\n",
        "\\begin{equation}\n",
        "\t\\nabla E(u) = -2div^{-}(C\\nabla^+u_{ij}) + 2\\beta(u - f)\n",
        "\\end{equation}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NV45ms7uGle9"
      },
      "outputs": [],
      "source": [
        "def denoise_energy_gradient(image: np.matrix,\n",
        "                            noise: np.matrix,\n",
        "                            coefficients: np.matrix,\n",
        "                            beta: float):\n",
        "    \"\"\"\n",
        "    Evaluates the denoising energy gradient from an image and the noisy data (see the guide)\n",
        "\n",
        "    :param image: target image (MxN)\n",
        "    :param noise: (MxN) noisy data for attachment term \n",
        "    :param coefficients: (MxN) coefficients image for regularization term\n",
        "    :param beta: (1x1) weight of attachment term\n",
        "\n",
        "    :return grade : (MxN) gradient of energy at u\n",
        "    \"\"\"\n",
        "    grade = 0.0\n",
        "    # TODO: Calculate the gradient of regularization term\n",
        "    gradient_i, gradient_j = im_fwd_gradient(image)\n",
        "    gradient_row = np.multiply(coefficients, gradient_i)\n",
        "    gradient_column = np.multiply(coefficients, gradient_j)\n",
        "\n",
        "    divg = im_bwd_divergence(gradient_row, gradient_column)\n",
        "\n",
        "    regularization = -2*(divg)\n",
        "\n",
        "    # TODO: Calculate the gradient of data attachment term\n",
        "    data_attachment = 2*beta*(image-noise)\n",
        "\n",
        "    # TODO: Calculate the gradient\n",
        "    grade = regularization+data_attachment\n",
        "    \n",
        "    return grade"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B88lJGSRGle9"
      },
      "source": [
        "### **3. Run the blocks with different denoising parameters (&beta; and c). In the report, show results with different values of &beta; and different images c (you can use the examples provided in the notebook). Describe the effect of these parameters in the result.**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gNKaD7BLGle-"
      },
      "source": [
        "**Load the image**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 299
        },
        "id": "-G8gLw-KGle-",
        "outputId": "8b362809-08b2-482b-f0f5-ebf4f3fe451a"
      },
      "outputs": [],
      "source": [
        "images_dir = os.path.abspath(\"/content/drive/MyDrive/Labs_2022/Lab2/images\")\n",
        "image_real = np.array(Image.open(os.path.join(images_dir, \"lena.pgm\")))\n",
        "plt.figure()\n",
        "plt.title(\"Real image\")\n",
        "plt.imshow(image_real, cmap = \"gray\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AY0MXxjRGle-"
      },
      "source": [
        "**Add noise to the image**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 299
        },
        "id": "scxpC3UCGle-",
        "outputId": "2f32aceb-244c-4627-f4ba-f421cabea3a0"
      },
      "outputs": [],
      "source": [
        "image_noisy = image_real + np.random.uniform(high = 50, size = image_real.shape)\n",
        "plt.figure()\n",
        "plt.title(\"Noisy image\")\n",
        "plt.imshow(image_noisy, cmap = \"gray\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gJXlcTixGle_"
      },
      "source": [
        "**Run the Gradient Descent**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_oq7ub_4Gle_",
        "outputId": "cfe90e43-2246-4b5c-9471-fdd445e849cb"
      },
      "outputs": [],
      "source": [
        "#USING ORIGINAL PARAMETERS\n",
        "# Define the gradient descent parameters\n",
        "callback_params = {\n",
        "    \"noise\": image_noisy,\n",
        "    \"coefficients\": np.ones_like(image_noisy),    # --> CHANGE THIS AND COMPARE\n",
        "    \"beta\": 0.5                                 # --> CHANGE THIS AND COMPARE\n",
        "}\n",
        "step_size = .01                                   # --> CHANGE THIS AND COMPARE\n",
        "max_iterations = 200                         # --> CHANGE THIS AND COMPARE\n",
        "tolerance = .1                                   # --> CHANGE THIS AND COMPARE\n",
        "\n",
        "# Run the gradient descent\n",
        "image_gd, _ = gradient_descent(callback = denoise_energy_gradient,\n",
        "                            callback_params = callback_params, \n",
        "                            initial_condition = image_noisy, \n",
        "                            step_size = step_size, \n",
        "                            max_iterations = max_iterations,\n",
        "                            tolerance = tolerance)\n",
        "\n",
        "\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "print(\"MSE: \", mean_squared_error(image_real, image_gd))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "O9AtlbNLzLm7",
        "outputId": "23c989b7-46d6-4b8f-e1dc-4c80fa23dd93"
      },
      "outputs": [],
      "source": [
        "##CODE TO FIND A GREAT CONFIGURATION IN ORDER TO GET A NICE OUTPUT IMAGE (DON'T EXECUTE - IT WILL RUN FOR A FEW HOURS)\n",
        "\n",
        "steps = [0.001,0.01,0.05,0.1,0.2,0.5]\n",
        "betas = [0.001,0.01,0.05,0.1,0.2,0.5,1,2,5]\n",
        "max_iterations = 100\n",
        "\n",
        "total_iterations = len(steps)*len(betas) \n",
        "min_mse = 10000000\n",
        "opt_b = 0\n",
        "opt_step = 0\n",
        "count = 0\n",
        "\n",
        "for beta in betas:\n",
        "  for step_size in steps:\n",
        "    callback_params = {\n",
        "        \"noise\": image_noisy,\n",
        "        \"coefficients\": np.ones_like(image_noisy),    # --> CHANGE THIS AND COMPARE\n",
        "        \"beta\": beta                                 \n",
        "    }\n",
        "    # Run the gradient descent\n",
        "    image_gd, _ = gradient_descent(callback = denoise_energy_gradient,\n",
        "                                callback_params = callback_params, \n",
        "                                initial_condition = image_noisy, \n",
        "                                step_size = step_size, \n",
        "                                max_iterations = max_iterations,\n",
        "                                tolerance = tolerance)\n",
        "    mse_ = mean_squared_error(image_real, image_gd)\n",
        "    if(mse_ < min_mse): # Saving min mse parameters\n",
        "      opt_img = image_gd\n",
        "      min_mse = mse_\n",
        "      opt_b = beta\n",
        "      opt_step = step_size\n",
        "    count+=1;\n",
        "    print(\" {}% MSE: {} Beta: {} Step: {}\".format(round(count/total_iterations,2)*100,round(mse_,3),beta,step_size))\n",
        "    #print(\"MIN MSE: {} Beta: {} Step: {}\".format(min_mse,opt_b,opt_step))\n",
        "    fig, (ax1, ax2, ax3) = plt.subplots(nrows = 1, ncols = 3, figsize = (20, 12))\n",
        "    ax1.imshow(image_real, cmap = \"gray\")\n",
        "    ax1.set_title(\"Real image\")\n",
        "    ax2.imshow(image_noisy, cmap = \"gray\")\n",
        "    ax2.set_title(\"Noisy image\")\n",
        "    ax3.imshow(image_gd, cmap = \"gray\")\n",
        "    ax3.set_title((\"MSE: {} B: {} S: {}\").format(mse_,beta,step_size))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7N8ReyIaGle_"
      },
      "source": [
        "**Show the results**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8EawMlK1RAqo",
        "outputId": "d57961a9-ac1a-4104-915a-c1a1565c188a"
      },
      "outputs": [],
      "source": [
        "#Configuration 1\n",
        "# Define the gradient descent parameters\n",
        "callback_params = {\n",
        "    \"noise\": image_noisy,\n",
        "    \"coefficients\": np.ones_like(image_noisy),    # --> CHANGE THIS AND COMPARE\n",
        "    \"beta\": 0.2                                 # --> CHANGE THIS AND COMPARE\n",
        "}\n",
        "step_size = .05                                   # --> CHANGE THIS AND COMPARE\n",
        "max_iterations = 200                          # --> CHANGE THIS AND COMPARE\n",
        "tolerance = .1                                   # --> CHANGE THIS AND COMPARE\n",
        "\n",
        "# Run the gradient descent\n",
        "image_gd_1, _ = gradient_descent(callback = denoise_energy_gradient,\n",
        "                            callback_params = callback_params, \n",
        "                            initial_condition = image_noisy, \n",
        "                            step_size = step_size, \n",
        "                            max_iterations = max_iterations,\n",
        "                            tolerance = tolerance)\n",
        "\n",
        "\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "print(\"MSE: \", mean_squared_error(image_real, image_gd_1))\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "henC7TilUXoY",
        "outputId": "8e2cdbf5-e8a9-4dfe-b876-8aed92336120"
      },
      "outputs": [],
      "source": [
        "#Configuration 2\n",
        "# Define the gradient descent parameters\n",
        "callback_params = {\n",
        "    \"noise\": image_noisy,\n",
        "    \"coefficients\": np.ones_like(image_noisy),    # --> CHANGE THIS AND COMPARE\n",
        "    \"beta\": 0.5                                 # --> CHANGE THIS AND COMPARE\n",
        "}\n",
        "step_size = .05                                   # --> CHANGE THIS AND COMPARE\n",
        "max_iterations = 200                          # --> CHANGE THIS AND COMPARE\n",
        "tolerance = .1                                   # --> CHANGE THIS AND COMPARE\n",
        "\n",
        "# Run the gradient descent\n",
        "image_gd_2, _ = gradient_descent(callback = denoise_energy_gradient,\n",
        "                            callback_params = callback_params, \n",
        "                            initial_condition = image_noisy, \n",
        "                            step_size = step_size, \n",
        "                            max_iterations = max_iterations,\n",
        "                            tolerance = tolerance)\n",
        "\n",
        "\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "print(\"MSE: \", mean_squared_error(image_real, image_gd_2))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 785
        },
        "id": "c0fnCC2-GlfA",
        "outputId": "09832e99-de21-4fd5-f171-b090e9593db6"
      },
      "outputs": [],
      "source": [
        "# Show the different images\n",
        "fig, (ax1, ax2, ax3) = plt.subplots(nrows = 1, ncols = 3, figsize = (20, 12))\n",
        "ax1.imshow(image_real, cmap = \"gray\")\n",
        "ax1.set_title(\"Real image\")\n",
        "ax2.imshow(image_noisy, cmap = \"gray\")\n",
        "ax2.set_title(\"Noisy image\")\n",
        "ax3.imshow(image_gd_1, cmap = \"gray\")\n",
        "ax3.set_title(\"Gradient Descent image\")\n",
        "\n",
        "fig, (ax1, ax2, ax3) = plt.subplots(nrows = 1, ncols = 3, figsize = (20, 12))\n",
        "ax1.imshow(image_real, cmap = \"gray\")\n",
        "ax1.set_title(\"Real image\")\n",
        "ax2.imshow(image_noisy, cmap = \"gray\")\n",
        "ax2.set_title(\"Noisy image\")\n",
        "ax3.imshow(image_gd_2, cmap = \"gray\")\n",
        "ax3.set_title(\"Gradient Descent image\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "q4Jk4yFTGlfA",
        "outputId": "d2b0da4d-23cf-4207-acef-3c3da1c29b7c"
      },
      "outputs": [],
      "source": [
        "# Show the absolute error among images\n",
        "noise = abs(image_real - image_noisy)\n",
        "denoised = abs(image_gd_1 - image_real)\n",
        "\n",
        "# Plot\n",
        "fig, (ax1, ax2) = plt.subplots(nrows = 1, ncols = 2, figsize = (20, 12))\n",
        "ax1.imshow(noise, cmap = \"gray\")\n",
        "ax1.set_title(\"Absolute error: real image vs noisy\")\n",
        "ax2.imshow(denoised, cmap = \"gray\")\n",
        "ax2.set_title(\"Absolute error: real image vs denoised\")\n",
        "\n",
        "noise = abs(image_real - image_noisy)\n",
        "denoised = abs(image_gd_2 - image_real)\n",
        "\n",
        "# Plot\n",
        "fig, (ax1, ax2) = plt.subplots(nrows = 1, ncols = 2, figsize = (20, 12))\n",
        "ax1.imshow(noise, cmap = \"gray\")\n",
        "ax1.set_title(\"Absolute error: real image vs noisy\")\n",
        "ax2.imshow(denoised, cmap = \"gray\")\n",
        "ax2.set_title(\"Absolute error: real image vs denoised\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "Z99GpI5LGlfA",
        "outputId": "803dcf36-6099-43b4-8f73-8c80a19cee6d"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize = (20, 12))\n",
        "plt.imshow(abs(image_gd_1 - image_noisy), cmap = \"gray\")\n",
        "plt.title('method noise')\n",
        "\n",
        "plt.figure(figsize = (20, 12))\n",
        "plt.imshow(abs(image_gd_2 - image_noisy), cmap = \"gray\")\n",
        "plt.title('method noise')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 299
        },
        "id": "uQhD7aS4Rhue",
        "outputId": "09cdedf4-0386-4a57-ab64-f3caee9e7115"
      },
      "outputs": [],
      "source": [
        "plt.figure()\n",
        "plt.title(\"Best output 1\")\n",
        "plt.imshow(image_gd_1, cmap = \"gray\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 299
        },
        "id": "f3R7e9MnRtAw",
        "outputId": "7b14f45e-e724-4d81-8d6b-a4734ce028ac"
      },
      "outputs": [],
      "source": [
        "plt.figure()\n",
        "plt.title(\"Best output 2\")\n",
        "plt.imshow(image_gd_2, cmap = \"gray\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 299
        },
        "id": "BOUehRy0XZUv",
        "outputId": "30e6428d-8471-4fe8-a88d-1383a122a92e"
      },
      "outputs": [],
      "source": [
        "plt.figure()\n",
        "plt.title(\"Real\")\n",
        "plt.imshow(image_real, cmap = \"gray\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oRqLfoEjPHoz"
      },
      "source": [
        "After changing some parameters we have seen that if we increase the beta,data attachment coefficient, the MSE decreases but the noise in the image remains. This is because the data attachment term penalizes high differences with the noisy image and beta is the weight of this term, which makes sense. To get the smaller MSE we have to choose a higher value of beta. Changing the tolerance only affects to the number of iterations, higher tolerance, less iterations will be done. We have changed the coefficients c by the identity matrix and the result was similar to the ones matrix. If we have a step higher than 0'2 the result image is a grey background. The best results, in our opinion, are with beta = 0'5 or beta = 0'2 (more smooth result, with less noise), step = 0'05, tolerance = 0.1, c = ones.  "
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "Gradient_and_Image_Denoising.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
